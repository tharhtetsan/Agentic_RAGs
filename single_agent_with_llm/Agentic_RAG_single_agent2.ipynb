{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'ths (Python 3.12.0)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n ths ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.agents import tool\n",
    "from crewai_tools import PDFSearchTool\n",
    "from crewai import Agent, Crew, Task,Process\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import crewai\n",
    "print(langchain.__version__)\n",
    "import langchain_community\n",
    "print(langchain_community.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use GROQ_API_KEY\n",
    "llm =ChatOpenAI(\n",
    "    openai_api_key=os.environ['GROQ_API_KEY'],\n",
    "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
    "    model_name=\"llama3-8b-8192\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "# OR Use Ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2:latest\")\n",
    "\n",
    "llm.invoke(\"The first man on the moon was ...\")\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom model and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default the tool uses OpenAI for both embeddings and summarization\n",
    "pdf_tool = PDFSearchTool(pdf='attentions_is_all_you_need.pdf',\n",
    "    config=dict(\n",
    "        llm=dict(\n",
    "            provider=\"groq\", # or google, openai, anthropic, llama2, ...\n",
    "            config=dict(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                # temperature=0.5,\n",
    "                # top_p=1,\n",
    "                # stream=true,\n",
    "            ),\n",
    "        ),\n",
    "        embedder=dict(\n",
    "            provider=\"huggingface\", # or openai, ollama, ...\n",
    "            config=dict(\n",
    "                model=\"BAAI/bge-small-en-v1.5\",\n",
    "                #task_type=\"retrieval_document\",\n",
    "                # title=\"Embeddings\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "# Web Search Tool\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test PDF tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pdf tools\n",
    "question  = \"How did self-attention mechanism evolve in large language models?\"\n",
    "print(pdf_tool.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Web search Tools\n",
    "question  = \"How did self-attention mechanism evolve in large language models?\"\n",
    "web_search_tool.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Router Tool\n",
    "@tool\n",
    "def simple_router_tool(question: str) -> str:\n",
    "    \"\"\"Routes queries to the appropriate resource.\"\"\"\n",
    "    if \"pdf\" in question.lower():\n",
    "        return \"pdf\"\n",
    "    else:\n",
    "        return \"web\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"How did self-attention mechanism evolve in large language models in pdf?\"\n",
    "simple_router_tool.run(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router Agent\n",
    "router_agent = Agent(\n",
    "    role=\"Router\",\n",
    "    goal=\"Determine if the query should go to the PDF or Web search.\",\n",
    "    backstory=(\n",
    "        \"You analyze user queries to decide whether to search a document \"\n",
    "        \"or perform a general web search.\"\n",
    "    ),\n",
    "    llm=llm, #default ChatGPT\n",
    ")\n",
    "\n",
    "\n",
    "router_task = Task(\n",
    "    description=(\n",
    "        \"Analyze the user question '{question}'. \"\n",
    "        \"If the question involves 'document', route it to the PDF search. \"\n",
    "        \"Otherwise, route it to web search.\"\n",
    "    ),\n",
    "    expected_output=\"Return 'pdf' for PDF search or 'web' for web search.\",\n",
    "    agent=router_agent,\n",
    "    tools=[simple_router_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_crew1 = Crew(\n",
    "    agents=[router_agent],\n",
    "    tasks=[router_task],\n",
    "    verbose=True,\n",
    "\n",
    ")\n",
    "\n",
    "test_input = {\"question\": \"What does the  say about machine learning?\"}\n",
    "result = rag_crew1.kickoff(inputs=test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever Agent\n",
    "retriever_agent = Agent(\n",
    "    role=\"Retriever\",\n",
    "    goal=\"Retrieve information from either a PDF or the web.\",\n",
    "    backstory=(\n",
    "        \"You are responsible for retrieving information from the appropriate \"\n",
    "        \"resource (PDF or web) based on the router's decision.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "retriever_task = Task(\n",
    "    description=(\n",
    "        \"Based on the routing decision ('pdf' or 'web'), use the relevant tool to retrieve information. \"\n",
    "        \"If 'pdf', use the PDF search tool. If 'web', use the web search tool.\"\n",
    "    ),\n",
    "    expected_output=\"Provide a clear, concise answer using the retrieved information.\",\n",
    "    context=[router_task],\n",
    "    tools=[pdf_tool, web_search_tool],\n",
    "    agent=retriever_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crew for the System\n",
    "simple_rag_crew = Crew(\n",
    "    agents=[router_agent, retriever_agent],\n",
    "    tasks=[router_task, retriever_task],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Input\n",
    "inputs = {\"question\": \"What does the say about machine learning?\"}\n",
    "\n",
    "#inputs ={\"question\":\"How does self-attention mechanism help large language models?\"}\n",
    "# Run the System\n",
    "response = simple_rag_crew.kickoff(inputs)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ths",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
