{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas langchain langchain-community sentence-transformers faiss-cpu \"transformers[agents]\"\n",
    "#!pip install \"transformers[agents]\"=='v4.44.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary modules\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from tqdm import tqdm\n",
    "from transformers.agents import Tool,HfEngine, ReactJsonAgent\n",
    "from huggingface_hub import InferenceClient\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the knowledge base\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m knowledge_base \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm-ric/huggingface_doc\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the knowledge base\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'knowledge_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mknowledge_base\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'knowledge_base' is not defined"
     ]
    }
   ],
   "source": [
    "knowledge_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 2647 documents from the knowledge base\n"
     ]
    }
   ],
   "source": [
    "# Convert dataset to Document objects\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
    "    for doc in knowledge_base\n",
    "]\n",
    "\n",
    "logger.info(f\"Loaded {len(source_docs)} documents from the knowledge base\")\n",
    "\n",
    "#source_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text splitter\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Splitting documents...\n",
      "100%|██████████| 2647/2647 [00:36<00:00, 73.32it/s] \n",
      "INFO:__main__:Processed 43181 unique document chunks\n"
     ]
    }
   ],
   "source": [
    "# Split documents and remove duplicates\n",
    "logger.info(\"Splitting documents...\")\n",
    "docs_processed = []\n",
    "unique_texts = {}\n",
    "for doc in tqdm(source_docs):\n",
    "    new_docs = text_splitter.split_documents([doc])\n",
    "    for new_doc in new_docs:\n",
    "        if new_doc.page_content not in unique_texts:\n",
    "            unique_texts[new_doc.page_content] = True\n",
    "            docs_processed.append(new_doc)\n",
    "\n",
    "logger.info(f\"Processed {len(docs_processed)} unique document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing embedding model...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: thenlper/gte-small\n",
      "INFO:__main__:Creating vector database...\n",
      "INFO:__main__:Vector database created successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "logger.info(\"Initializing embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "\n",
    "# Create the vector database\n",
    "logger.info(\"Creating vector database...\")\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=docs_processed,\n",
    "    embedding=embedding_model,\n",
    "    distance_strategy=DistanceStrategy.COSINE,\n",
    ")\n",
    "\n",
    "logger.info(\"Vector database created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./faiss_index\"\n",
    "vectordb.save_local(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: thenlper/gte-small\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "# load Vector DB\n",
    "faiss_path = \"./faiss_index\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "\n",
    "vectordb= FAISS.load_local(faiss_path,embeddings=embedding_model,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Retriever Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"text\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"text\"\n",
    "\n",
    "    def __init__(self, vectordb, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vectordb = vectordb\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        docs = self.vectordb.similarity_search(\n",
    "            query,\n",
    "            k=7,\n",
    "        )\n",
    "\n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            [f\"===== Document {str(i)} =====\\n\" + doc.page_content for i, doc in enumerate(docs)]\n",
    "        )\n",
    "\n",
    "# Create an instance of the RetrieverTool\n",
    "retriever_tool = RetrieverTool(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result :  \n",
      "Retrieved documents:\n",
      "===== Document 0 =====\n",
      ".fast.ai/). Join the fast.ai [Discord](https://discord.com/invite/YKrxeNn) and [forums](https://forums.fast.ai/). It is a guarantee that you will learn by being part of their community!===== Document 1 =====\n",
      "## Community===== Document 2 =====\n",
      "<a href=\"https://huggingface.co/new-space?template=Panel-Org/panel-template\">\n",
      "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-panel.png\" style=\"width:70%\"> </a>\n",
      "\n",
      "\n",
      "## 🌐 Join Our Community\n",
      "The Panel community is vibrant and supportive, with experienced developers and data scientists eager to help and share their knowledge. Join us and connect with us:===== Document 3 =====\n",
      "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XvSGPZFEjDY\" title=\"YouTube video player\"\n",
      "frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;\n",
      "picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "<Tip>\n",
      "\n",
      "To share a model with the community, you need an account on [huggingface.co](https://huggingface.co/join). You can also join an existing organization or create a new one.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "## Repository features===== Document 4 =====\n",
      "* **I am just starting to contribute. Can I be a fellow?**\n",
      "\n",
      "Fellows are nominated based on their open-source and community contributions. If you want to participate in the Fellowship, the best way to start is to begin contributing! If you are a student, the [Student Ambassador Program](https://huggingface.co/blog/ambassadors) might be more suitable for you (the application deadline is June 13, 2022).\n",
      "\n",
      "\n",
      "* **Where and how can I contribute?**\n",
      "  \n",
      "It depends on your interests. Here are some ideas of areas where you can contribute, but you should work on things that get **you** excited!===== Document 5 =====\n",
      "To be able to share your model with the community there are three more steps to follow:\n",
      "\n",
      "1️⃣ (If it's not already done) create an account to HF ➡ https://huggingface.co/join\n",
      "\n",
      "2️⃣ Sign in and get your authentication token from the Hugging Face website.\n",
      "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
      "\n",
      "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n",
      "\n",
      "- Copy the token\n",
      "- Run the cell below and paste the token===== Document 6 =====\n",
      "If you have questions, you can ask them in the [Hugging Face forum](https://discuss.huggingface.co/) so the Hugging Face community can help you out and others can benefit from seeing the discussion. You can also join our [Discord](https://discord.gg/YRAq8fMnUG) server to talk with us and the entire Hugging Face community.\n"
     ]
    }
   ],
   "source": [
    "result = retriever_tool.forward(\"How can I join the community?\")\n",
    "print(\"result : \",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nllm_engine = HfEngine(\"meta-llama/Meta-Llama-3-8B-Instruct\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the language model engine\n",
    "\"\"\"\n",
    "llm_engine = HfEngine(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nfrom openai import OpenAI\\n\\nfrom typing import List, Dict\\nfrom transformers.agents.llm_engine import MessageRole, get_clean_message_list\\nfrom huggingface_hub import InferenceClient\\n\\nopenai_role_conversions = {\\n    MessageRole.TOOL_RESPONSE: MessageRole.USER,\\n}\\n\\n\\nclass OpenAIEngine:\\n    def __init__(self, model_name=\"gpt-4o\"):\\n        self.model_name = model_name\\n        self.client = OpenAI(\\n            api_key=\"YOUR-API-KEY\"#os.getenv(\"OPENAI_API_KEY\"),\\n        )\\n\\n    def __call__(self, messages, stop_sequences=[]):\\n        messages = get_clean_message_list(messages, role_conversions=openai_role_conversions)\\n\\n        response = self.client.chat.completions.create(\\n            model=self.model_name,\\n            messages=messages,\\n            stop=stop_sequences,\\n            temperature=0.5,\\n        )\\n        return response.choices[0].message.content\\n\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### IF YOU WANT TO USE OPENAI\n",
    "\"\"\"\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from typing import List, Dict\n",
    "from transformers.agents.llm_engine import MessageRole, get_clean_message_list\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "openai_role_conversions = {\n",
    "    MessageRole.TOOL_RESPONSE: MessageRole.USER,\n",
    "}\n",
    "\n",
    "\n",
    "class OpenAIEngine:\n",
    "    def __init__(self, model_name=\"gpt-4o\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(\n",
    "            api_key=\"YOUR-API-KEY\"#os.getenv(\"OPENAI_API_KEY\"),\n",
    "        )\n",
    "\n",
    "    def __call__(self, messages, stop_sequences=[]):\n",
    "        messages = get_clean_message_list(messages, role_conversions=openai_role_conversions)\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            stop=stop_sequences,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process with LLM Studio\n",
    "from openai import OpenAI\n",
    "\n",
    "class LLMStudioEngine:\n",
    "     def __init__(self) -> None:\n",
    "         pass\n",
    "     \n",
    "     def __call__(self,messages, stop_sequences=[]):\n",
    "  \n",
    "        client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "                {\"role\": \"user\", \"content\": messages}\n",
    "                ]\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
    "            messages=messages,\n",
    "            stop=stop_sequences,\n",
    "            temperature=0.7,\n",
    "            )\n",
    "        \n",
    "        return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test LLM\n",
    "llm_engine = LLMStudioEngine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = ReactJsonAgent(tools=[retriever_tool], llm_engine=llm_engine, max_iterations=4, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the agent\n",
    "def run_agentic_rag(question: str) -> str:\n",
    "    enhanced_question = f\"\"\"Using the information contained in your knowledge base, which you can access with the 'retriever' tool,\n",
    "give a comprehensive answer to the question below.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "If you cannot find information, do not give up and try calling your retriever again with different arguments!\n",
    "Make sure to have covered the question completely by calling the retriever tool several times with semantically different queries.\n",
    "Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\n",
    "\n",
    "Question:\n",
    "{question}\"\"\"\n",
    "\n",
    "    return agent.run(enhanced_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\n",
      "give a comprehensive answer to the question below.\n",
      "Respond only to the question asked, response should be concise and relevant to the question.\n",
      "If you cannot find information, do not give up and try calling your retriever again with different arguments!\n",
      "Make sure to have covered the question completely by calling the retriever tool several times with semantically different queries.\n",
      "Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\n",
      "\n",
      "Question:\n",
      "How can I push a model to the Hub?\u001b[0m\n",
      "\u001b[38;20mSystem prompt is as follows:\u001b[0m\n",
      "\u001b[38;20mYou are an expert assistant who can solve any task using JSON tool calls. You will be given a task to solve as best you can.\n",
      "To do so, you have been given access to the following tools: 'retriever', 'final_answer'\n",
      "The way you use the tools is by specifying a json blob, ending with '<end_action>'.\n",
      "Specifically, this json should have an `action` key (name of the tool to use) and an `action_input` key (input to the tool).\n",
      "\n",
      "The $ACTION_JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. It should be formatted in json. Do not try to escape special characters. Here is the template of a valid $ACTION_JSON_BLOB:\n",
      "{\n",
      "  \"action\": $TOOL_NAME,\n",
      "  \"action_input\": $INPUT\n",
      "}<end_action>\n",
      "\n",
      "Make sure to have the $INPUT as a dictionary in the right format for the tool you are using, and do not put variable names as input if you can find the right values.\n",
      "\n",
      "You should ALWAYS use the following format:\n",
      "\n",
      "Thought: you should always think about one action to take. Then use the action as follows:\n",
      "Action:\n",
      "$ACTION_JSON_BLOB\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $ACTION_JSON_BLOB must only use a SINGLE action at a time.)\n",
      "\n",
      "You can use the result of the previous action as input for the next action.\n",
      "The observation will always be a string: it can represent a file, like \"image_1.jpg\".\n",
      "Then you can use it as input for the next action. You can do it for instance as follows:\n",
      "\n",
      "Observation: \"image_1.jpg\"\n",
      "\n",
      "Thought: I need to transform the image that I received in the previous observation to make it green.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"image_transformer\",\n",
      "  \"action_input\": {\"image\": \"image_1.jpg\"}\n",
      "}<end_action>\n",
      "\n",
      "To provide the final answer to the task, use an action blob with \"action\": \"final_answer\" tool. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"final_answer\",\n",
      "  \"action_input\": {\"answer\": \"insert your final answer here\"}\n",
      "}<end_action>\n",
      "\n",
      "\n",
      "Here are a few examples using notional tools:\n",
      "---\n",
      "Task: \"Generate an image of the oldest person in this document.\"\n",
      "\n",
      "Thought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"document_qa\",\n",
      "  \"action_input\": {\"document\": \"document.pdf\", \"question\": \"Who is the oldest person mentioned?\"}\n",
      "}<end_action>\n",
      "Observation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\n",
      "\n",
      "\n",
      "Thought: I will now generate an image showcasing the oldest person.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"image_generator\",\n",
      "  \"action_input\": {\"text\": \"\"A portrait of John Doe, a 55-year-old man living in Canada.\"\"}\n",
      "}<end_action>\n",
      "Observation: \"image.png\"\n",
      "\n",
      "Thought: I will now return the generated image.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"final_answer\",\n",
      "  \"action_input\": \"image.png\"\n",
      "}<end_action>\n",
      "\n",
      "---\n",
      "Task: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\n",
      "\n",
      "Thought: I will use python code evaluator to compute the result of the operation and then return the final answer using the `final_answer` tool\n",
      "Action:\n",
      "{\n",
      "    \"action\": \"python_interpreter\",\n",
      "    \"action_input\": {\"code\": \"5 + 3 + 1294.678\"}\n",
      "}<end_action>\n",
      "Observation: 1302.678\n",
      "\n",
      "Thought: Now that I know the result, I will now return it.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"final_answer\",\n",
      "  \"action_input\": \"1302.678\"\n",
      "}<end_action>\n",
      "\n",
      "---\n",
      "Task: \"Which city has the highest population , Guangzhou or Shanghai?\"\n",
      "\n",
      "Thought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\n",
      "Action:\n",
      "{\n",
      "    \"action\": \"search\",\n",
      "    \"action_input\": \"Population Guangzhou\"\n",
      "}<end_action>\n",
      "Observation: ['Guangzhou has a population of 15 million inhabitants as of 2021.']\n",
      "\n",
      "\n",
      "Thought: Now let's get the population of Shanghai using the tool 'search'.\n",
      "Action:\n",
      "{\n",
      "    \"action\": \"search\",\n",
      "    \"action_input\": \"Population Shanghai\"\n",
      "}\n",
      "Observation: '26 million (2019)'\n",
      "\n",
      "Thought: Now I know that Shanghai has a larger population. Let's return the result.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"final_answer\",\n",
      "  \"action_input\": \"Shanghai\"\n",
      "}<end_action>\n",
      "\n",
      "\n",
      "Above example were using notional tools that might not exist for you. You only have acces to those tools:\n",
      "\n",
      "- retriever: Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\n",
      "    Takes inputs: {'query': {'type': 'text', 'description': 'The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.'}}\n",
      "\n",
      "- final_answer: Provides a final answer to the given problem.\n",
      "    Takes inputs: {'answer': {'type': 'text', 'description': 'The final answer to the problem'}}\n",
      "\n",
      "Here are the rules you should always follow to solve your task:\n",
      "1. ALWAYS provide a 'Thought:' sequence, and an 'Action:' sequence that ends with <end_action>, else you will fail.\n",
      "2. Always use the right arguments for the tools. Never use variable names in the 'action_input' field, use the value instead.\n",
      "3. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.\n",
      "4. Never re-do a tool call that you previously did with the exact same parameters.\n",
      "\n",
      "Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\n",
      "\u001b[0m\n",
      "\u001b[38;20m===== New step =====\u001b[0m\n",
      "===== Calling LLM with this last message: =====\n",
      "{'role': <MessageRole.USER: 'user'>, 'content': 'Task: Using the information contained in your knowledge base, which you can access with the \\'retriever\\' tool,\\ngive a comprehensive answer to the question below.\\nRespond only to the question asked, response should be concise and relevant to the question.\\nIf you cannot find information, do not give up and try calling your retriever again with different arguments!\\nMake sure to have covered the question completely by calling the retriever tool several times with semantically different queries.\\nYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\\n\\nQuestion:\\nHow can I push a model to the Hub?'}\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\u001b[31;20mError in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 915, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/xd/3z5vvpds0zxf_pypxd4cn3d80000gn/T/ipykernel_91346/770280807.py\", line 15, in __call__\n",
      "    completion = client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 758, in direct_run\n",
      "    step_logs = self.step()\n",
      "                ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 917, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\n",
      "\u001b[38;20m===== New step =====\u001b[0m\n",
      "===== Calling LLM with this last message: =====\n",
      "{'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': '[OUTPUT OF STEP 0] Error: Error in generating llm output: Error code: 400 - {\\'error\\': \"Invalid \\'content\\': \\'content\\' objects must have a \\'type\\' field that is either \\'text\\' or \\'image_url\\' Got \\'object\\'.\"}.\\nNow let\\'s retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\\n'}\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\u001b[31;20mError in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 915, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/xd/3z5vvpds0zxf_pypxd4cn3d80000gn/T/ipykernel_91346/770280807.py\", line 15, in __call__\n",
      "    completion = client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 758, in direct_run\n",
      "    step_logs = self.step()\n",
      "                ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 917, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\n",
      "\u001b[38;20m===== New step =====\u001b[0m\n",
      "===== Calling LLM with this last message: =====\n",
      "{'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': '[OUTPUT OF STEP 1] Error: Error in generating llm output: Error code: 400 - {\\'error\\': \"Invalid \\'content\\': \\'content\\' objects must have a \\'type\\' field that is either \\'text\\' or \\'image_url\\' Got \\'object\\'.\"}.\\nNow let\\'s retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\\n'}\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\u001b[31;20mError in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 915, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/xd/3z5vvpds0zxf_pypxd4cn3d80000gn/T/ipykernel_91346/770280807.py\", line 15, in __call__\n",
      "    completion = client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 758, in direct_run\n",
      "    step_logs = self.step()\n",
      "                ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 917, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\n",
      "\u001b[38;20m===== New step =====\u001b[0m\n",
      "===== Calling LLM with this last message: =====\n",
      "{'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': '[OUTPUT OF STEP 2] Error: Error in generating llm output: Error code: 400 - {\\'error\\': \"Invalid \\'content\\': \\'content\\' objects must have a \\'type\\' field that is either \\'text\\' or \\'image_url\\' Got \\'object\\'.\"}.\\nNow let\\'s retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\\n'}\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\u001b[31;20mError in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 915, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/xd/3z5vvpds0zxf_pypxd4cn3d80000gn/T/ipykernel_91346/770280807.py\", line 15, in __call__\n",
      "    completion = client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 758, in direct_run\n",
      "    step_logs = self.step()\n",
      "                ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 917, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\n",
      "\u001b[31;20mReached max iterations.\u001b[0m\n",
      "NoneType: None\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can I push a model to the Hub?\n",
      "Answer: Error in generating final llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"How can I push a model to the Hub?\"\n",
    "answer = run_agentic_rag(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\n",
      "give a comprehensive answer to the question below.\n",
      "Respond only to the question asked, response should be concise and relevant to the question.\n",
      "If you cannot find information, do not give up and try calling your retriever again with different arguments!\n",
      "Make sure to have covered the question completely by calling the retriever tool several times with semantically different queries.\n",
      "Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\n",
      "\n",
      "Question:\n",
      "How to install Huggingface lib?\u001b[0m\n",
      "\u001b[38;20mSystem prompt is as follows:\u001b[0m\n",
      "\u001b[38;20mYou are an expert assistant who can solve any task using JSON tool calls. You will be given a task to solve as best you can.\n",
      "To do so, you have been given access to the following tools: 'retriever', 'final_answer'\n",
      "The way you use the tools is by specifying a json blob, ending with '<end_action>'.\n",
      "Specifically, this json should have an `action` key (name of the tool to use) and an `action_input` key (input to the tool).\n",
      "\n",
      "The $ACTION_JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. It should be formatted in json. Do not try to escape special characters. Here is the template of a valid $ACTION_JSON_BLOB:\n",
      "{\n",
      "  \"action\": $TOOL_NAME,\n",
      "  \"action_input\": $INPUT\n",
      "}<end_action>\n",
      "\n",
      "Make sure to have the $INPUT as a dictionary in the right format for the tool you are using, and do not put variable names as input if you can find the right values.\n",
      "\n",
      "You should ALWAYS use the following format:\n",
      "\n",
      "Thought: you should always think about one action to take. Then use the action as follows:\n",
      "Action:\n",
      "$ACTION_JSON_BLOB\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $ACTION_JSON_BLOB must only use a SINGLE action at a time.)\n",
      "\n",
      "You can use the result of the previous action as input for the next action.\n",
      "The observation will always be a string: it can represent a file, like \"image_1.jpg\".\n",
      "Then you can use it as input for the next action. You can do it for instance as follows:\n",
      "\n",
      "Observation: \"image_1.jpg\"\n",
      "\n",
      "Thought: I need to transform the image that I received in the previous observation to make it green.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"image_transformer\",\n",
      "  \"action_input\": {\"image\": \"image_1.jpg\"}\n",
      "}<end_action>\n",
      "\n",
      "To provide the final answer to the task, use an action blob with \"action\": \"final_answer\" tool. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"final_answer\",\n",
      "  \"action_input\": {\"answer\": \"insert your final answer here\"}\n",
      "}<end_action>\n",
      "\n",
      "\n",
      "Here are a few examples using notional tools:\n",
      "---\n",
      "Task: \"Generate an image of the oldest person in this document.\"\n",
      "\n",
      "Thought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"document_qa\",\n",
      "  \"action_input\": {\"document\": \"document.pdf\", \"question\": \"Who is the oldest person mentioned?\"}\n",
      "}<end_action>\n",
      "Observation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\n",
      "\n",
      "\n",
      "Thought: I will now generate an image showcasing the oldest person.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"image_generator\",\n",
      "  \"action_input\": {\"text\": \"\"A portrait of John Doe, a 55-year-old man living in Canada.\"\"}\n",
      "}<end_action>\n",
      "Observation: \"image.png\"\n",
      "\n",
      "Thought: I will now return the generated image.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"final_answer\",\n",
      "  \"action_input\": \"image.png\"\n",
      "}<end_action>\n",
      "\n",
      "---\n",
      "Task: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\n",
      "\n",
      "Thought: I will use python code evaluator to compute the result of the operation and then return the final answer using the `final_answer` tool\n",
      "Action:\n",
      "{\n",
      "    \"action\": \"python_interpreter\",\n",
      "    \"action_input\": {\"code\": \"5 + 3 + 1294.678\"}\n",
      "}<end_action>\n",
      "Observation: 1302.678\n",
      "\n",
      "Thought: Now that I know the result, I will now return it.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"final_answer\",\n",
      "  \"action_input\": \"1302.678\"\n",
      "}<end_action>\n",
      "\n",
      "---\n",
      "Task: \"Which city has the highest population , Guangzhou or Shanghai?\"\n",
      "\n",
      "Thought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\n",
      "Action:\n",
      "{\n",
      "    \"action\": \"search\",\n",
      "    \"action_input\": \"Population Guangzhou\"\n",
      "}<end_action>\n",
      "Observation: ['Guangzhou has a population of 15 million inhabitants as of 2021.']\n",
      "\n",
      "\n",
      "Thought: Now let's get the population of Shanghai using the tool 'search'.\n",
      "Action:\n",
      "{\n",
      "    \"action\": \"search\",\n",
      "    \"action_input\": \"Population Shanghai\"\n",
      "}\n",
      "Observation: '26 million (2019)'\n",
      "\n",
      "Thought: Now I know that Shanghai has a larger population. Let's return the result.\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"final_answer\",\n",
      "  \"action_input\": \"Shanghai\"\n",
      "}<end_action>\n",
      "\n",
      "\n",
      "Above example were using notional tools that might not exist for you. You only have acces to those tools:\n",
      "\n",
      "- retriever: Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\n",
      "    Takes inputs: {'query': {'type': 'text', 'description': 'The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.'}}\n",
      "\n",
      "- final_answer: Provides a final answer to the given problem.\n",
      "    Takes inputs: {'answer': {'type': 'text', 'description': 'The final answer to the problem'}}\n",
      "\n",
      "Here are the rules you should always follow to solve your task:\n",
      "1. ALWAYS provide a 'Thought:' sequence, and an 'Action:' sequence that ends with <end_action>, else you will fail.\n",
      "2. Always use the right arguments for the tools. Never use variable names in the 'action_input' field, use the value instead.\n",
      "3. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.\n",
      "4. Never re-do a tool call that you previously did with the exact same parameters.\n",
      "\n",
      "Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\n",
      "\u001b[0m\n",
      "\u001b[38;20m===== New step =====\u001b[0m\n",
      "===== Calling LLM with this last message: =====\n",
      "{'role': <MessageRole.USER: 'user'>, 'content': 'Task: Using the information contained in your knowledge base, which you can access with the \\'retriever\\' tool,\\ngive a comprehensive answer to the question below.\\nRespond only to the question asked, response should be concise and relevant to the question.\\nIf you cannot find information, do not give up and try calling your retriever again with different arguments!\\nMake sure to have covered the question completely by calling the retriever tool several times with semantically different queries.\\nYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\\n\\nQuestion:\\nHow to install Huggingface lib?'}\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\u001b[31;20mError in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 915, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/xd/3z5vvpds0zxf_pypxd4cn3d80000gn/T/ipykernel_91346/770280807.py\", line 15, in __call__\n",
      "    completion = client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 758, in direct_run\n",
      "    step_logs = self.step()\n",
      "                ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 917, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\n",
      "\u001b[38;20m===== New step =====\u001b[0m\n",
      "===== Calling LLM with this last message: =====\n",
      "{'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': '[OUTPUT OF STEP 0] Error: Error in generating llm output: Error code: 400 - {\\'error\\': \"Invalid \\'content\\': \\'content\\' objects must have a \\'type\\' field that is either \\'text\\' or \\'image_url\\' Got \\'object\\'.\"}.\\nNow let\\'s retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\\n'}\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\u001b[31;20mError in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 915, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/xd/3z5vvpds0zxf_pypxd4cn3d80000gn/T/ipykernel_91346/770280807.py\", line 15, in __call__\n",
      "    completion = client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 758, in direct_run\n",
      "    step_logs = self.step()\n",
      "                ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 917, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\n",
      "\u001b[38;20m===== New step =====\u001b[0m\n",
      "===== Calling LLM with this last message: =====\n",
      "{'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': '[OUTPUT OF STEP 1] Error: Error in generating llm output: Error code: 400 - {\\'error\\': \"Invalid \\'content\\': \\'content\\' objects must have a \\'type\\' field that is either \\'text\\' or \\'image_url\\' Got \\'object\\'.\"}.\\nNow let\\'s retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\\n'}\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\u001b[31;20mError in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 915, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/xd/3z5vvpds0zxf_pypxd4cn3d80000gn/T/ipykernel_91346/770280807.py\", line 15, in __call__\n",
      "    completion = client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 758, in direct_run\n",
      "    step_logs = self.step()\n",
      "                ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 917, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\n",
      "\u001b[38;20m===== New step =====\u001b[0m\n",
      "===== Calling LLM with this last message: =====\n",
      "{'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': '[OUTPUT OF STEP 2] Error: Error in generating llm output: Error code: 400 - {\\'error\\': \"Invalid \\'content\\': \\'content\\' objects must have a \\'type\\' field that is either \\'text\\' or \\'image_url\\' Got \\'object\\'.\"}.\\nNow let\\'s retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\\n'}\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\u001b[31;20mError in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 915, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/xd/3z5vvpds0zxf_pypxd4cn3d80000gn/T/ipykernel_91346/770280807.py\", line 15, in __call__\n",
      "    completion = client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/openai/_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 758, in direct_run\n",
      "    step_logs = self.step()\n",
      "                ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ths/lib/python3.12/site-packages/transformers/agents/agents.py\", line 917, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\n",
      "\u001b[31;20mReached max iterations.\u001b[0m\n",
      "NoneType: None\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to install Huggingface lib?\n",
      "Answer: Error in generating final llm output: Error code: 400 - {'error': \"Invalid 'content': 'content' objects must have a 'type' field that is either 'text' or 'image_url' Got 'object'.\"}.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"How to install Huggingface lib?\"\n",
    "answer = run_agentic_rag(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ths",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
